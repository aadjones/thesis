\chapter[Transform-based compression of fluid subspaces]{Transform-based compression of fluid subspaces}
\label{chap:chap4}
Note: A large portion of this chapter has previously appeared as~\cite{Jones:2016:CFS}.
\bigskip\bigskip

In the previous chapter, we discussed the potential for subspace methods to accelerate the computational cost of physics-based simulations. However, a significant drawback of the subspace approach is the time/memory tradeoff: the speed increase comes at a cost of much larger memory requirements. Specifically, subspace simulations can easily consume dozens of gigabytes of memory when dealing with high-resolution scenes. In this chapter, we discuss a compression method to reduce the memory footprint of subspace methods by an order of magnitude. Using such a compression scheme will allow us to compute longer and more complex scenes on powerful computers, while at the same time giving us the ability to compute scenes of reasonable complexity on laptop computers.

\section{Previous Work}
Since memory consumption is a known challenge with subspace techniques, other research has focused on reducing the memory footprint of these simulations. In the applications of sound \cite{Langlois:2014:ECM} and blendshape matrices \cite{Seo:2011:CDM}, compression techniques have been developed; however, we are unaware of analogous research in subspace fluid simulation. In the work of Wicke et al.~\cite{Wicke:2009}, a modular fluid basis is used that can be tiled throughout the domain. However, our approach is complementary, as the modular tiles themselves could be further compressed by applying our algorithm.

\section{Background}
\subsection{Data Compression Preliminaries}
The general problem of data compression at first blush seems daunting: how do we reduce the memory footprint of a set of data without losing information? Fortunately, the sets of data that we typically work with are not random, but rather contain many patterns and redundancies. Exploiting these redundancies is the key to any successful data compression algorithm.

Data compression algorithms can be divided into two distinct families: lossless and lossy. A lossless compression algorithm is able to take an input signal, compress it, and reconstruct the exact same input signal. A simple example is run-length coding. For instance, to represent 100 white pixels, followed by 200 black pixels, followed by an another 50 white pixels, we can code the sequence (`w', 100), (`b', 200), (`w', 50), rather than coding the 350 pixels individually. Decoding is straightforward and retains all information. The ZIP file format is a familiar example of widely-used lossless data compression, based on methods pioneered by Lempel and Ziv \cite{ziv1977universal}.

Lossless compression can be very powerful when applied to original data with many redundancies; however, it has strict limitations based on the mathematics of information theory \cite{shannon1998mathematical}. Every file, no matter how redundant, contains a certain amount of information. Thus, for instance, after using run-length coding once on a file that previously contained many long sequential redundancies, trying to use it a second time on the newly compressed file will actually {\em increase} the file size, since there will no longer be any runs.

Lossy compression, by contrast, may not be able to reconstruct the exact same input signal. Its applications tend to focus on perceptual data, such as images, video, and sound. Although its reconstructions are not perfectly faithful, if the technique is soundly based on the limits of human perception, the results are virtually indistinguishable. The JPEG file format for images is a widely-used lossy data compression algorithm \cite{wallace1992jpeg}.

Data streams of images oftentimes contain more information than a human can reasonably resolve. For example, a picture might contain fine details that are indistinguishable from their smoothed counterpoints, or subtle color gradients that are indistinguishable from coarser ones. The key to most lossy data compression algorithms is finding a way of transforming the original information into a domain that captures these characteristics more naturally. These techniques are called {\em transform coding}.

\subsection{Mathematical Preliminaries}
In order to describe transform coding formally, we will require several notions from linear algebra and analysis.
\subsubsection{Vector Spaces}
The basic framework of most of our data manipulation can be expressed through the notion of vector spaces. We are familiar with the physical notion of vectors in two- or three-dimensional space as quantities with both a magnitude and a direction, often depicted graphically with an arrow, as in Figure~\ref{fig:vec2d}. For convenience, we often represent vectors using coordinates. We can view the coordinate representation as a decomposition into fundamental unit vectors pointing in the $x$ and $y$ direction. These are called the {\em basis} vectors. Any arbitrary vector can be decomposed uniquely into a combination of these basis vectors. 

\begin{figure}[h]
\centering
\begin{asy}
	arrowhead MyHead;
	MyHead.head=new path(path g, position position=EndPoint, pen p=currentpen,
                          real size=0, real angle=arrowangle) {
                          arrowfactor=3;
  if(size == 0) size=MyHead.size(p);
  bool relative=position.relative;
  real position=position.position.x;
  if(relative) position=reltime(g,position);
  path r=subpath(g,position,0);
  pair x=point(r,0);
  real t=arctime(r,size);
  pair y=point(r,t);
  path base=arrowbase(r,y,t,size);
  path left=rotate(-angle,x)*r;
  path right=rotate(angle,x)*r;
  real[] T=arrowbasepoints(base,left,right);
  return subpath(left,0,T[0])--point(r,.5*t)--subpath(right,T[1],0)&cycle;
};

size(250);
pair O, P, Q, X, Y;
O = (0, 0);
P = (4, 0);
Q = (4, 3);
X = (5, 0);
Y = (0,4);
draw(O--X);
draw(O--Y);

draw(O--Q, 1.0bp+red, Arrow(MyHead));
label("$\mathbf{v}$", O--Q, NW);
draw(O--P, 1.5bp+blue, Arrow(MyHead));
label("$\mathbf{v}_x$", O--P, S);
draw(P--Q, 1.5bp+blue, Arrow(MyHead));
label("$\mathbf{v}_y$", P--Q, E);
\end{asy}
\caption{\em A 2D vector $\vv$ decomposed into its orthogonal components $\vv_x$ and $\vv_y$.}
\label{fig:vec2d}
\end{figure}

Vectors can be added or subtracted, producing another vector. They can also be multiplied by a scalar (i.e.,~a number in $\R$ or $\C$), yielding another vector. The formalization of this idea is the concept of a {\em vector space}, which allows for a more abstract treatment of vectors. For example, an image represented on a computer as a sequence of $N$ pixel values can be regarded as a vector living in the vector space $\R^N$. This abstraction allows us to bring the tools of linear algebra to bear on a variety of seemingly different applications.

\subsubsection{Inner Products} \label{sec:innerprod}
An {\em inner product}, or dot product, is a binary operation $\langle \cdot, \cdot \rangle$ between two vectors that yields a scalar as an output. The most familiar example is in $\R^2$, where, given two vectors $\vv = (v_x, v_y)$ and $\ww = (w_x, w_y)$, we have $\langle \vv, \ww \rangle = v_x w_x + v_y w_y$. Inner products add a notion of geometry to a vector space through angles and length. When an inner product between two vectors is $0$, the vectors are orthogonal, and the inner product of a vector with itself gives the square of its length. For example, the basis vectors $\ee_x = (1, 0)$ and $\ee_y = (0, 1)$ are orthogonal, and the vector $\vv = (3, 4)$ has squared length $\langle \vv, \vv \rangle = 3^2 + 4^2 = 5^2$, which matches with the usual Euclidean notion of squared length. Inner products also are closely related to the projection of one vector onto another. For example, if we take an arbitrary vector $\vv = (v_x, v_y)$ and take its inner product with the unit basis vector $\ee_y = (0, 1)$, the result is the component $v_y$ in the $y$ direction. In general, given a set of unit basis vectors which are mutually orthogonal (known as an {\em orthonormal} basis), we can compute the various components of arbitrary vectors by calculating their inner products with the corresponding basis vector.

Besides the familiar inner product in $\R^2$, more general inner products exist in other vector spaces. The canonical inner product in $\C^n$ is given by $\langle \vv, \ww \rangle = \displaystyle \sum_{k=1}^{n}v_k\overline{w_k}$, where the overbar denotes the complex conjugate. The conjugate may seem strange, but is necessary to preserve the notion that taking an inner product of a vector with itself should give the squared length of the vector. 

\subsubsection{The Discrete Fourier Transform} \label{sec: DFT}
The {\em Discrete Fourier Transform}, or DFT, is a transformation that maps one vector in $\C^{N}$ to another vector $\C^{N}$. Given a vector $\xx = \left(x_0, x_1, \dots, x_{N-1}\right)$, the DFT maps $\xx$ to the output $\XX = \left(X_0, X_1, \dots, X_{N-1}\right)$, where

\begin{equation}
	X_k = \sum_{n=0}^{N-1}x_n e^{-i2\pi n k/N}, \ \ k = 0, 1, \dots, N-1
\label{eq:dft}
\end{equation}

This definition, as written, appears somewhat unmotivated, so we give a brief geometrical interpretation of the DFT in terms of inner products and roots of unity. 
To begin, we write $\omega = e^{i2 \pi / N}$ as the first $N$-th root of unity\footnote{That is, $\omega$ satisfies $\omega^N = 1$.}. Then the $N$ powers of $\omega$ comprise the entire set of $N$-th roots of unity: $\omega^0, \omega^1, \omega^2, \dots, \omega^{N-1}$. If we collect these into a single vector $\boldomega$, we have 

\begin{equation}
	\boldomega = \left(1, \omega^1, \omega^2, \dots, \omega^{N-1}\right) \in \C^N. 
\end{equation}

We can also consider the $N$ powers of $\boldomega$:
\begin{equation}
	\boldomega^k = \left(1, \omega^k, \omega^{2k}, \dots, \omega^{(N-1)k}\right), \ \ k = 0, 1, \dots, N-1
\end{equation}
The collection of $N$ vectors $\{\boldomega^0, \boldomega^1, \dots, \boldomega^{N-1}\}$ forms an orthogonal\footnote{Orthogonal, but not orthonormal, since each basis vector has magnitude $\sqrt{N} \neq 1$.} basis for $\C^N$, which we shall call the Fourier basis. As such, given an arbitrary vector $\xx = \left(x_0, x_1, \dots, x_{N-1}\right)$, we can compute its representation in the Fourier basis by projecting $\xx$ against each of the new basis functions using the complex inner product. More concretely, the new representation $\XX = \left(X_0, X_1, \dots, X_{N-1}\right)$ in the Fourier basis is given by 

\begin{equation}
	X_k = \frac{1}{\sqrt{N}} \langle \xx, \boldomega^k \rangle, \ \ k = 0, 1, \dots, N-1
\end{equation}
Recalling the definition of the complex inner product given in \S\ref{sec:innerprod}, this can be expand to agree with the original definition~\ref{eq:dft} up to a constant factor:

\begin{align}
	X_k &= \frac{1}{\sqrt{N}}\sum_{n=0}^{N-1}x_n \overline{\omega^{nk}}\\
	        &= \frac{1}{\sqrt{N}}\sum_{n=0}^{N-1}x_n \overline{\left(e^{i 2 \pi / N}\right)^{nk}}\\
	        &= \frac{1}{\sqrt{N}}\sum_{n=0}^{N-1}x_n e^{-i2\pi n k/N}, \ \ k = 0, 1, \dots, N-1
\end{align}

This interpretation of projecting against the Fourier basis will prove more useful than the explicit formula for having an intuition about what the DFT is doing under the hood.
In fact, using this interpretation, we can even write the DFT as the following $N \times N$ change-of-basis matrix $\fancyF$, with each of the $N$ columns given by the corresponding $\boldomega^k$ vector:

\begin{align}
	\fancyF &= \begin{pmatrix}
    		\vertbar & \ \  \vertbar & \ \ \ \ \ \ \ \ \  \vertbar & & \vertbar \\
    		\boldomega^0  & \ \   \boldomega^1  & \ \ \ \ \ \ \ \ \  \boldomega^2 & \ \ \ \ \ \dots & \ \ \ \ \ \ \boldomega^{N-1} \\
 		\vertbar & \ \  \vertbar & \ \ \ \ \ \ \ \ \  \vertbar & & \vertbar 
  	\end{pmatrix} \\	
	&= \begin{pmatrix}
	1 & 1 & 1 & \dots & 1 \\
	1 & \omega^{-1} & \omega^{-2} & \dots & \omega^{-(N-1)} \\
	1 & \omega^{-2} & \omega^{-4} & \dots & \omega^{-2(N-1)} \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	1 & \omega^{-(N-1)} & \omega^{-2(N-1)} & \dots & \omega^{-{(N-1)}^2}
	\end{pmatrix}	
\end{align}

Thus, the DFT of a vector $\xx \in \C^N$ can now simply be defined by the matrix-vector multiplication $\fancyF \xx = \XX$. In particular, since we already observed that the $N$ column vectors of this matrix form on orthogonal basis for $\C^N$, by normalizing the DFT matrix to make each column have unit magnitude, we obtain the {\em unitary} form of the DFT:

\begin{equation}
	\fancyF_{\textrm{unitary}} = \frac{1}{\sqrt{N}} \fancyF
\end{equation}

Being unitary means that $\fancyF_{\textrm{unitary}}$ preserves inner products. In other words, for any vectors $\vv, \ww \in \C^N$, we have

\begin{equation}
	\langle \fancyF_{\textrm{unitary}}\vv, \fancyF_{\textrm{unitary}}\ww \rangle = \langle \vv, \ww \rangle
\end{equation}
We shall exploit this property later on in \S\ref{sec:decompress}.

The asymptotic run-time of the DFT of a vector $\xx \in \C^N$, if implemented as a matrix-vector multiply, would be $O(N^2)$, as we must compute $N$ multiply-adds for each of the $N$ columns. However, the well-known fast Fourier transform (FFT) algorithm, by recursively expressing the $N$-point DFT in terms of smaller $N/2$-point DFTs, is able to compute the same result in $O(N \log{N})$ time, making it much more practical for large inputs.~\cite{1965-cooley}

The multidimensional DFT applies takes multidimensional arrays of inputs rather than vectors. For example, the two-dimensional DFT of the array $\xx \in \C^{M \times N}$ is given by
the array $\XX \in \C^{M \times N}$ whose entries are
\begin{equation}
	\XX_{u,v} = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} \xx_{m, n} e^{-i 2 \pi \left(um + vn\right)}
\end{equation}

In the most general case, given a $d$-dimensional array $\xx \in C^{N_1 \times N_2 \times \cdots \times N_d}$, its d-dimensional DFT is given by the array $\XX \in \C^{N_1 \times N_2 \times \cdots \times N_d}$ whose entries are
\begin{equation}
	\XX_{u_1, u_2, \dots, u_d} = \sum_{n_1=0}^{N_1-1} \sum_{n_2=0}^{N_2-1} \cdots \sum_{n_d = 0}^{N_d -1} \xx_{n_1, n_2, \dots, n_d} e^{-i 2 \pi \left(u_1 n_1 + u_2 n_2 + \cdots + u_d n_d\right)}
\end{equation}
In practice, however, we will stick to using $d=2$ or $d=3$.
\subsection{The JPEG Coding Algorithm} \label{sec:jpeg}
The JPEG coding scheme is a form of lossy, transform compression for digital images. Because our subspace fluid compression scheme is based on many of the techniques used in JPEG, we shall explore the implementation of JPEG in some detail to better motivate our own coding algorithm. 

The point of departure in JPEG is a digital image file, which comprises a two-dimensional array of pixel values. (For simplicity, we will assume a grayscale image.) The basic redundancy that JPEG exploits is that for most images, the majority of the ``energy'' is packed into lower spatial frequencies. In other words, generally speaking, there are not many extremely sharp changes between hues in a typical image. (Notable exceptions include medical imaging, for which JPEG is typically not applied.) 

So, to begin, suppose we have a two-dimensional grayscale image of resolution $800 \times 800$, as in Figure \ref{fig:charles}. While over the space of the entire image, there may be sharp changes between hues, if we subdivide the image into smaller regions, within each individual region, there are sharp changes much more rarely. Hence, the first step of JPEG is to subdivide the image into many smaller regions. The typical ``sweet spot'' chosen is $8 \times 8$ blocks, so in this case, there will be $100 \cdot 100 = 10000$ such blocks. Figure  \ref{fig:charles} shows one such block in the center of the whole image. The grayscale values (as unsigned $8$-bit integers between $0$ and $255$) for this block $\BB$ are as follows:

\begin{figure}
	\includegraphics[width=0.5\textwidth]{chap4/figures/charles_gray.png}
	\includegraphics[width=0.5\textwidth]{chap4/figures/charles_block.png}
	\caption{{\em{\bf Left:} The original $800 \times 800$ image.} {\em{\bf Right:} An $8 \times 8$ sub-block zoomed in.}}
	\label{fig:charles}
\end{figure}

\begin{align}
	\BB = \begin{pmatrix}
	56 & 42 &	46 &	43 &	46 & 48 &	43 &	48\\
	45 &	47 &	59 & 	47 &	57 &	50 &	50 &	65\\
	48 &	59 &	37 &	41 &	45 &	52 &	67 &	74\\
	51 &	39 &	47 &	50 &	55 &	70 &	54 &	36\\
	39 &	66 &	59 &	72 &	88 &	49 &	26 &	20\\
	66 &	69 &	76 &	75 &	50 &	24 &	25 &	25\\
	70 &	78 &	79 &	38 &	20 &	23 &	22 &	20\\
	75 &	56 &	27 &	23 &	23 &	23 &	26 &	56
	\end{pmatrix}
	\label{eq:subblock}
	\end{align}

To exploit the redundancy of the image in the frequency domain, we now transform each block according to a two-dimensional discrete cosine transform (2D-DCT). The discrete cosine transform, with some massaging, can actually be regarded as a particular case of the discrete Fourier transform, so conceptually, we can regard it in the same spirit. The resulting data transforms $\BB$ into $\textnormal{DCT}\left(\BB\right) = \widehat{\BB}$ as follows:

\begin{equation}
	\widehat{\BB} = \begin{pmatrix}
	388.13   &   48.51   &   3.43   &   -5.62   &   3.63   &   -10.51   &   1.42   &   2.12\\
	21.90     &	 -65.26   & -10.63  &  10.88   &   -0.39  &	1.66     & 	2.68	  &   2.75\\
	-27.13    &   5.92     &   53.83 &   -8.29   &	 5.35   &	1.02	    & 11.21   &   0.80\\
	6.26	      &   45.24   & -45.46  & -13.37  &	 1.97   &	3.12     &	0.07    &   6.32\\
	-12.63    &	-15.30    & -11.94  &	28.36   &	11.38  &	-4.05    & 1.56	   &  -3.43\\
	-8.00	      &   9.89     &   14.93  &  5.72     &	-20.23 &   16.34   &  9.22     &   -2.35\\
	0.82	      &   -8.88    &   17.96  &	-0.44    &  13.89   &    4.13	    & -10.33   & -7.66\\
	0.16	      &   3.34     &   -1.24   & 	-2.58    & 	-5.01	   &   -15.33  & -11.65   &  -0.71	
	\end{pmatrix}
\label{eq:dct2}
\end{equation}

\begin{figure}
\label{fig:jpeg8}
	\centering
	\includegraphics[width=0.5\textwidth]{chap4/figures/jpeg8.png}
	\caption{\em The $64$ 2D-DCT basis vectors for an $8 \times 8$ domain. Any $8 \times 8$ image can be expressed as a linear combination of these vectors. For example, the $8 \times 8$ array in \ref{eq:dct2} gives the corresponding weights that would generate the original sub-block $\BB$ in \ref{eq:subblock}.}
\end{figure}

Next, we {\em dampen} the frequencies by point-wise dividing the resulting set of values by a pre-determined damping array, $\QQ$, given by Equation \ref{eq:q50}. This array was determined empirically through perceptual limits and is given in \cite{wallace1992jpeg}.
\begin{equation}
	\QQ = \begin{pmatrix}
	16 & 11 & 10 & 16 & 24 & 40 & 51 & 61\\
	12 & 12 & 14 & 19 & 26 & 58 & 60 & 55\\
	14 & 13 & 16 & 24 & 40 & 57 & 69 & 56\\
	14 & 17 & 22 & 29 & 51 & 87 & 80 & 62\\
	18 & 22 & 37 & 56 & 68 & 109 & 103 & 77\\
	24 & 35 & 55 & 64 & 81 & 104 & 113 & 92\\
	49 & 64 & 78 & 87 & 103 & 121 & 120 & 101\\
	72 & 92 & 95 & 98 & 112 & 100 & 103 & 99	
	\end{pmatrix}
\label{eq:q50}	
\end{equation}

The goal of this procedure is to dampen enough of the frequencies to values small enough that, upon integer rounding, they become zero. This allows for compression gains, albeit at the cost of information loss. Equation \ref{eq:round} shows the result.

\begin{equation}
	\textnormal{Round}\left(\frac{\widehat{\BB}}{\QQ}\right) = \begin{pmatrix}
	24 &	4 &	0 &	0 &	0 &	0 &	0 &	0\\
	2 &	-5 &	-1 &	1 &	0 &	0 &	0 &	0\\
	-2 &	0 &	3 &	0 &	0 &	0 &	0 &	0\\
	0 &	3 &	-2 &	0 &	0 &	0 &	0 &	0\\
	-1 &	-1 &	0 &	1 &	0 &	0 &	0 &	0\\
	0 &	0 &	0 &	0 &	0 &	0 &	0 &	0\\
	0 &	0 &	0 &	0 &	0 &	0 &	0 &	0\\
	0 &	0 &	0 &	0 &	0 &	0 &	0 &	0
	\end{pmatrix}
\label{eq:round}
\end{equation}

Note how many of the entires have been dampened to zero, as desired. Next, we scan through this array in a zigzag fashion, as demonstrated by Figure \ref{fig:zigzag}.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{chap4/figures/zigzag.png}
	\caption{\em The zigzag scan from northwest to southeast corner. The goal is to catch long consecutive strings of zeroes.}
	\label{fig:zigzag}
\end{figure}

This allows us to catch long strings of zeroes. In this case, the final $31$ entries are all zeroes, so run-length coding will prove to be highly effective. Although the full JPEG standard includes several additional wrinkles, these are beyond the scope of the current discussion.

The decoding procedure is a straightforward reversal of each of the coding steps: for each block, we first decode the run-length code, we unzigzag the data into the correct shape, we undampen it by multiplying by $\QQ$, and finally, we compute the inverse two-dimensional discrete cosine transform (inverse 2D DCT). Figure \ref{fig:blockrecon} shows the JPEG-decoded block side-by-side with its original counterpart. The overall average error in this block comes out to approximately $7$ values per pixel (on a $256$-value grayscale). Figure \ref{fig:comp_comparison} shows the whole image compressed. The average error across the whole image comes out to about $2$ bits per pixel, with an overall compression ratio of around $8 : 1$. Despite artifacts being noticeable at the block level when zoomed in, as in Figure \ref{fig:blockrecon}, the overall image has no visible artifacts. 

Lossy compression typically has a ``sweet spot''---that is, a level of compression that achieves a significant data reduction without introducing noticeable artifacts. We also see in Figure \ref{fig:comp_comparison} the effect of going beyond this sweet spot and destroying the image quality: here, despite the attractive $100 : 1$ compression ratio, the image is unacceptably distorted.

\begin{figure}

	\centering
	\includegraphics[scale=20]{chap4/figures/charles_block.png}
	\includegraphics[scale=20]{chap4/figures/charles_block_reconstructed.png}
	\caption{{\em{\bf Left:} The original, uncompressed $8\times8$ data block.} {\em{\bf Right:} The same $8 \times 8$ block after undergoing JPEG compression at quality $50$ percent. Observe the general smoothing, which is an artifact of the information lost during the dampening and rounding stage.}}
\label{fig:blockrecon}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{chap4/figures/charles_gray_50.jpg}
	\includegraphics[width=0.4\textwidth]{chap4/figures/charles_gray_5.jpg}
	\caption{{\em{\bf Left:} A compression ratio of $8 : 1$ is achieved using the $50$ percent quality damping array with no visible artifacts.} {\em{\bf Right:} A compression ratio of $100 : 1$ is achieved using the $5$ percent quality damping array; however, there are many visible artifacts.}}
\label{fig:comp_comparison}
\end{figure}

\section{A Subspace Compression Scheme}
\label{sec:Algorithm}
Recall from Chapter \ref{chap:chap3} the subspace projection matrix $\UU$ that creates a 
heavy memory footprint. A compression method to reduce its size would alleviate this potential computational bottleneck. However, in order for the subspace simulations to run smoothly, we also must ensure that the compression method efficiently supports the following
three operations:
\begin{itemize}
\item \textbf{Projection:} The time needed to compute the full $\UU^T \uu = \utilde$ matrix-vector product should not prohibitively increase due to the presence of a decompressor.
\item \textbf{Dense reconstruction:} Conversely, $\UU \utilde = \uu$ must also be fast.
\item \textbf{Batched random access:} In order to support sparse reconstruction, it should be possible to query the velocity field at a set of random points on the simulation grid. In addition to efficient sparse reconstruction, this feature is also needed to support certain types of time integration, e.g.~cubature-based semi-Lagrangian schemes \cite{Kim2013}.
\end{itemize}
With these requirements in mind, we can design our codec.
\subsection{Compression Basis Selection}
\label{sec:laplacian}
In order to design a compression scheme for the velocity fields that comprise the columns of the subspace projecton matrix $\UU$ , we must first select a transform basis that would ideally result in extremely sparse fields. Both discrete cosine transform (DCT) \cite{Yeo:1995:VRD} and wavelet \cite{guthe2002,treib12turbulence} bases have been successfully used in the past to compress scalar volumes, so they are promising candidates for velocity fields. We choose to use DCT because we observe that in the special case of Laplacian Eigenfunctions \cite{deWitt:2012}, they actually yield ideal compression. The eigenfunctions inside a closed 3D box take the general form:
\begin{equation}
\begin{split}
\uu_x(k_1,k_2,k_3) &= \kappa_x \sin(k_1 x) \cos(k_2 y) \cos(k_3 z) \\
\uu_y(k_1,k_2,k_3) &= \kappa_y \cos(k_1 x) \sin(k_2 y) \cos(k_3 z) \\
\uu_z(k_1,k_2,k_3) &= \kappa_z \cos(k_1 x) \cos(k_2 y) \sin(k_3 z),
\end{split}
\end{equation}
where $\uu_x, \uu_y,$ and $\uu_z$ respectively represent the $x$, $y$, and $z$ components of a velocity field. The $k_i$ coefficients determine the frequency content of the field, and the three $\kappa$ terms are scaling coefficients that are derived from $k_i$.

\begin{figure}
\label{fig:laplacians}
\includegraphics[width=\textwidth]{chap4/figures/laplacian_sparsity.png}
\caption{\em An eigenfunction of the Laplacian operator transforming into a delta function in frequency space.}
\end{figure}

We make the straightforward observation that the spatially varying components of each of these functions are purely trigonometric functions, so by applying appropriately interleaved DCTs and discrete sine transforms (DSTs) to these fields, they can be reduced to delta functions, regardless of their spatial frequency. In the notation of Long and Reinhard \cite{long2009}, if we use $\fancyF_\textrm{SCC}$ to denote a DST in the $x$ direction and DCTs in the $y$ and $z$ directions, we obtain,
\begin{equation}
\fancyF_\textrm{SCC}\left[\kappa_x\sin(k_1 x) \cos(k_2 y) \cos(k_3 z)\right] = \kappa_x\delta(k_1, k_2, k_3),
\end{equation}
where $\delta(k_1, k_2, k_3)$ is a delta function in SCC frequency space located at the $(k_1, k_2, k_3)$ grid cell. Similar transforms, e.g.~$\fancyF_\textrm{CSC}$ and $\fancyF_\textrm{CCS}$ can be used to generate delta functions for $\uu_y$ and $\uu_z$. Thus, any eigenfunction can be compressed down to three integers (the $k_i$s) and three floats (the $\kappa$ terms).

Asymptotically, this mixed DCT/DST losslessly transforms an $O(N)$ eigenfunction down to $O(1)$; no sparser representation is possible. This result only applies to the ideal case of analytic divergence-free velocity fields defined on the interior of a box. However, the high compression it achieves is encouraging, and motivates our further use of DCT to compress more general divergence-free fields.

\subsection{DCT-Based Compression}

Following from the previous discussion, we design a DCT-based, JPEG-like compression scheme. Each column of $\UU$ represents a vector field, and the columns are usually constructed using an SVD. Since this SVD has already minimized the amount of redundant information between columns, we compress them separately. Each column contain $x$, $y$ and $z$ velocity components, and we extract each of these components and compress them separately. Thus, if we describe the encoding procedure for a single scalar field, it can be applied to each velocity component of each column of $\UU$.

Analogously to JPEG, given a 3D scalar field, we decompose it into small blocks of size $b \times b \times b$, adding continuous extra padding in the case that one or more resolutions are not evenly divisible by $b$. We then perform a 3D-DCT on each block. In anticipation of quantization, the result is then normalized so that the largest frequency-domain value maps to the largest signed value for a 32-bit integer.

\noindent \textbf{Adaptive Quantization:} After transforming the signal to the frequency domain and normalizing the coefficients, as discussed in \S\ref{sec:jpeg}, the JPEG coding scheme then performs an element-wise division of the coefficients using a 2D quantization matrix in order to increase the likelihood that they will quantize to zero. In the JPEG standard, this matrix is adjusted depending on the quality setting; higher quality settings will have lower values to preserve more detail after dividing by the matrix, while lower quality settings will have higher values in the matrix to suppress more coefficients. For example, we previously saw in Equation \ref{eq:q50} the JPEG matrix corresponding to 50\% quality.

We index the matrix entries $\QQ(u,v)$ such that the upper-left corner, $\QQ(0,0) = 16$, corresponds to the DC (i.e.,~direct current, or zero frequency) component.  The entries of $\QQ$ were obtained from perceptual data \cite{Sayood:2012:JPEG}, and in general, higher frequency components have larger entries in order to suppress these coefficients which tend to carry less information than those in the lower frequencies.

Since we are not working with 2D color data but rather with 3D velocity fields, we need to construct a 3D version of this matrix, $\QQQ \in \R^{b \times b \times b}$. A close inspection of all of the complete matrix corresponding to Eqn.~\ref{eq:q50} suggests that $\QQ(u,v) \propto u + v$, so a straightforward first attempt is:
\begin{equation}
\QQQ(u,v,w) = 1 + u + v + w.
\end{equation}
Other applications \cite{Yeo:1995:VRD} have used similar reasoning to arrive at similar matrices. However, while the 2D case has a suite of $\QQ$ matrices at its disposal that correspond to different levels of perceived visual quality, this data does not generalize to non-chromatic, 3D velocity data.

We instead propose to automatically generate a variety of different $\QQQ$ matrices during the compression stage. For different $b \times b \times b$ blocks, the energy is likely to reside in different frequencies, so we generate a one-parameter family of matrices,
\begin{equation}
\QQQ(u,v,w) = (1 + u + v + w)^\gamma,
\end{equation}
where $\gamma$ is a parameter that is adjusted per block. 
%Linear models are also possible, e.g.~$\QQQ(u,v,w) = \gamma\;(1 + u + v + w)$, but preliminary experiments
%found that this model was too simple to yield useful compressions. 
Analogous to the 2D case, the user specifies a quality parameter $p$. In 3D, we interpret $p$ as the percentage of the original energy\footnote{That is, $L_2$ norm.} that should be preserved. Each block then performs a bisection search over the range $\gamma \in [0, n]$, where $n = 32$ is the number of bits that were used for normalization prior to quantization. Higher values of $n$ are essentially meaningless, as they damp everything except the DC component to zero. In practice, we found that this bisection search terminates within a very small tolerance of the desired energy preservation after at most $8$ iterations.

This approach provides a custom quantization matrix for each block while maintaining an approximately constant energy loss per block. Important high-frequency components are preserved when they are present, while smoother, low-frequency blocks are still aggressively compressed. This block-varying value $\gamma$ must then be computed by the encoder and provided to the decoder in the encoded bytestream. For an $8 \times 8 \times 8$ block, the memory footprint of a single additional scalar $\gamma$ per block is negligible. We compare this strategy to a uniform non-adaptive quantization approach in \S\ref{sec:Results}.

\noindent \textbf{Flattening and Encoding:} After quantization, we convert the 3D array into a 1D array and perform run-length encoding \cite{Yeo:1995:VRD,Sayood:2012:JPEG}. No novel strategy needs to be devised for this component. The 3D to 1D conversion is performed in a zigzag pattern that is a straightforward 3D extension of the usual 2D JPEG ordering, which tries to group coefficients with similar sizes together in the bytestream.  In our case, the entries of $\QQQ(u,v,w)$ are arranged in increasing order of their sum $u + v + w$, which effectively clusters components with approximately the same frequency. The results are then run-length encoded in order to discover long runs of zeros.

\begin{figure}
\includegraphics[width=\textwidth]{chap4/figures/compression_flowchart.png}
\caption{\em An overview of the encoding pipeline.}
\label{fig:flowchart}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{chap4/figures/zigzag3d.png}
\caption{\em The 3D zigzag scan order. We move along slice planes of increasing index sum $c = 1 + u + v + w$.}
\label{fig:zigzag3d}
\end{figure}


\subsection{Subspace Decompression}
\label{sec:decompress}

\noindent \textbf{Batched Random Access:} The block-wise compression scheme we have described supports the batched random access requirements described in the beginning of \S\ref{sec:Algorithm} at runtime. For a single random access, the block containing the cell of interest is decompressed, which means the other $b \times b \times b - 1$ entries are potentially decoded needlessly. However, the coherency of the underlying incompressible flow tends to cluster cells of interest in the same blocks, so we did not find that this extra overhead created a major bottleneck.

The decompression proceeds in two stages, where an initial pass assembles the batch of requested cells and determines which blocks need to be decompressed. A second pass then decompresses the actual blocks. By consolidating the cell requests, it is guaranteed that no block is ever redundantly decompressed twice.

\noindent \textbf{Projection and Reconstruction:} The fast projection and reconstruction requirements from \S\ref{sec:Algorithm} are not as straightforward. A na\"{i}ve strategy is to decompress the entire matrix $\UU$ for each projection and reconstruction. For a given block size $B = b \times b \times b$, this results in $\frac{3N \times r}{B}$ DCTs and IDCTs at every timestep. These transforms dominate the running time (Table \ref{tab:naive-vs-sparse}), and largely negate the performance gains of the subspace approach. While memory savings are achieved, the speed-memory tradeoff is unacceptable.

However, we observe that both of the matrix-vector products $\UU\utilde = \uu$ and $\UU^T\uu = \utilde$ can be performed {\em sparsely in the frequency domain}. The projection operator then only performs a DCT on $\uu$, not all $r$ columns of $\UU$. This operation is permissible because the DCT is a unitary transform, and therefore preserves inner products, as seen in \S\ref{sec: DFT}. Specifically, if $\xx$ and $\yy$ are vectors in the spatial domain and $\xhat$ and $\yhat$ are their counterparts in the frequency domain, $\langle \xx, \yy \rangle = \langle \xhat, \yhat \rangle$.

We define the following notation to describe the advantages of this approach. The frequency domain version of a quantity is denoted with a hat, e.g., $\UU$ with DCT applied to each column is $\Uhat$. The {\em lossy, compressed} version of $\Uhat$, where near-zero values have been quantized to zero, is denoted $\Chat$. The spatial domain version of $\Chat$ is correspondingly $\CC$. In essence, our compression scheme has introduced the approximations $\UU \approx \CC$ and $\Uhat \approx \Chat$.

Using the unitary property discussed in \S\ref{sec:innerprod}, we can see that if we use DCT to transform $\uu$ to $\uhat$, then $\UU^T\uu = \utilde$ is equivalent to $\Uhat^T\uhat = \utilde$. Using the compressed versions will yield a result slightly different from $\qq$, but the same relation holds: $\CC^T\uu = \Chat^T\uhat \approx \utilde$. The na\"{i}ve approach spends most of its time transforming $\Chat$ to $\CC$, but by constructing $\uhat$, we can avoid this stage altogether. Replacing the IDCT over all $r$ columns of $\Chat$ with a single DCT of $\uu$ is significant, because even for a modest $r = 10$, the number of transforms is reduced by an order of magnitude. Additionally, the $\Chat^T\uhat$ product can now exploit the sparsity of $\Chat$ that was discovered by the compression stage. Since $\Chat$ is static over the lifetime of a simulation, the location of all the zero entries can be cached, and these multiplies can be skipped. 

A fast reconstruction strategy then follows due to the linearity of the DCT: $\Chat\utilde \approx \uhat$. The sparsity of $\Chat$ can again be exploited here, as each column $\Chat$ is scaled by an entry of $\utilde$, and all the multiplies with respect to zeroes can again be skipped. Once $\uhat$ is known, an IDCT can be performed on it once, and IDCTs over all $r$ columns of $\Chat$ is again avoided.

For a $\CC$ matrix containing ${3N \times r}$ non-zero entries, after taking the complexity of the DCTs and IDCTs from \S\ref{sec: DFT} into account, na\"{i}ve projection and reconstruction each take $O\left(3N \times r +\frac{3N \times r}{B} B \log B\right) = O\left((3N \times r)\log B\right)$ time. Using our approach, a $\Chat$ containing $S$ non-zero entries instead takes $O\left(S + \frac{3N}{B} B \log B\right) = O\left(S + 3N \log B\right)$ time. The $r$ factor has been removed as a multiplier of $\log B$, and replaced with the additive $S$ term. As seen in Table \ref{tab:naive-vs-sparse}, this results in speedups that approach an order of magnitude.

\subsection{Discussion}

We have described one possible scheme for compressing subspace fluid basis matrices. Several initially promising possibilities were also investigated, but ultimately discarded.

The motivating example from \S\ref{sec:laplacian} uses mixed DST and DCT to achieve ideal compression, whereas we only use DCT. The use of DST was investigated as well, but was not found to give superior results. Unless the velocity values along a block border are all exactly zero, i.e.~the block has Dirichlet boundaries along all its walls, the DCT consistently yields superior results, as shown in Figure \ref{fig:interleaved}.

\begin{figure}
	\centering
	\includegraphics[height=0.5\textwidth]{chap4/figures/transform_comparison.eps}
	\caption{\em A comparison of compression results using the $8$ possible cosine/sine intereavings through each of the three spatial dimensions. `C' and `S' are abbreviations for a cosine and sine transform,
	respectively. Wee see that the `CCC' transform yields the most compressed result with the smallest output file size.}
	\label{fig:interleaved}
\end{figure}

The $\UU$ matrix is usually constructed using an SVD \cite{Treuille:2006:MRF,Kim2013} or an eigenanalysis \cite{deWitt:2012,Liu:2015:MVF}. Therefore, corresponding singular values or eigenvalues are usually available for each column of $\UU$. These values could be used to guide the compressor, e.g.,~by allowing columns with unimportant singular values $\sigma_i$ to be compressed more aggressively. However, we found that the relationship between $\sigma_i$ and visual quality is not straightforward. Especially during re-simulation, columns that had unimportant $\sigma_i$ during the initial analysis can obtain large coefficients in $\utilde$. In such cases, the aggressive compression can become visible.

Prior to compression, an additional SVD could be run on each $b \times b \times b$ block in $\UU$ to determine if there is a superior coordinate system for compression other than the canonical $x$, $y$, and $z$ axes. However, the per-block rotation that this introduces breaks the fast matrix-vector multiply described in \S\ref{sec:decompress}. Thus, we put this aside in favor of the fast multiplies, but a method that supports both operations is an interesting direction for future work.

\begin{figure}
		\centering
		\includegraphics[height=0.5\textwidth]{chap4/figures/blockSpatial}
		\includegraphics[height=0.5\textwidth]{chap4/figures/blockFreq}
%		\vspace*{-1em}
		\caption{{\em{\bf Left:} An $8 \times 8 \times 8$ block of one of the velocity fields obtained from the Singular Value Decomposition.} {\em{\bf Right:} The same $8 \times 8 \times 8$ velocity field block in the frequency domain after taking a $3$D Discrete Cosine Transform. The sparsity not only allows for transform compression but also frequency-domain subspace projection speedups.}}
		\label{fig:sparseFreq}
\end{figure}

\section{Results}
\label{sec:Results}

We tested our compression scheme on several subspace fluid re-simulation scenarios generated by the open-source package Zephyr \cite{Kim2013}. The fluid simulation data were generated using a Preconditioned Conjugate Gradient (PCG) solver with a Modified Incomplete Cholesky preconditioner \cite{Bridson:2015}. The codebase was implemented in C++ and tests were run on a 12-core, 2.66 GHz Mac Pro with 96 GB RAM. For the DCT and IDCT, we used FFTW \cite{FFTW05}, and Eigen \cite{eigenweb} was used for other linear algebra operations.

In all of our simulations, we set $b = 8$, so $8 \times 8 \times 8$ blocks were used. Block sizes of $b = 4$ and $b = 16$ were also tested, but we found that smaller blocks redundantly captured the same low frequency information, while larger blocks lessened the likelihood of finding smooth regions that could be compressed aggressively. Figure \ref{fig:blockcomparison} summarizes these results.

\begin{figure}
	\centering
	\includegraphics[height=0.5\textwidth]{chap4/figures/blockSizeSparsityComparison.eps}
	\caption{\em Sparsity comparison of using different block sizes. Using $b=8$ leads to sparser, and hence more compressible results.}
	\label{fig:blockcomparison}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|c||c|c|c|} \hline
 & Na\"{i}ve, e.g.,~$\CC\utilde$ & Sparse, e.g.,~$\Chat\uhat$ & Speedup\\
\hline
Plume & 72.0s & 8.7s & 8.3X\\
\hline
Sphere & 74.9s & 7.6s & 9.9X \\
\hline
Fan & 72.6s & 12.9s & 5.6X \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:naive-vs-sparse}{\em Timings of na\"{i}ve projections vs. sparse projections. The sparse projection is significantly faster, and dramatically reduces the overhead of using the compressed representation of $\UU$. These timings represent the average time to perform {\em both} the projection and reconstruction stages in a single timestep.}}
\end{table}

In all of our subspace simulations, we used the matrix-vector product strategy from \S\ref{sec:decompress}, which accelerated the computation significantly (Table \ref{tab:naive-vs-sparse}). Without this acceleration, the subspace simulations ran almost at parity with the original full-space simulations, invalidating any speed advantages of the subspace approach. On average, our sparse product ran roughly 3\textendash4 times as slow as the uncompressed matrix vector product. Asymptotically, our sparse product can have a superior running time, as it does not need to touch all $3N \times r$ entries in the matrix. Our scenes did not achieve sufficient sparsity to demonstrate this superiority, but we expect that as compression methods improve, multiplying against $\Chat$ will eventually become faster than multiplying with $\CC$.

\begin{table*}
\begin{center}
\scriptsize
\setlength\tabcolsep{1.5pt}
\begin{tabular}{|c||c|c|c|s|c|c|} \hline

\textbf{Plume} & Uncompressed & 6 : 1 & 8  : 1 & \textbf{11 : 1} & 13 : 1 & 22 : 1 \\ 
\hline
Time per frame & 4.5s & 19.9s & 17.9s & 16.1s & 15.2s & 12.8s \\
Compression preprocess & N/A & 02h 07m 55s & 01h 53m 48s & 02h 07m 55s & 02h 12m 17s & 02h 15m 59s \\
\hline
\end{tabular}

\begin{tabular}{|c||c|c|s|c|c|c|} \hline
\textbf{Sphere} & Uncompressed & 10 : 1 & \textbf{14 : 1} & 16 : 1 & 23 : 1 & 30 : 1 \\ 
\hline
Time per frame & 5.4s & 17.1 & 15.1s & 14.4s & 13.0s & 12.2s \\
Compression preprocess & N/A & 02h 01m 38s & 02h 20m 14s & 02h 07m 59s & 02h 34m 30s & 02h 27m 59s \\
\hline
\end{tabular}

\begin{tabular}{|c||c|c|s|c|c|c|} \hline
\textbf{Fan} & Uncompressed & 5 : 1 & \textbf{6 : 1} & 8 : 1 & 11 : 1 & 29 : 1 \\ 
\hline
Time per frame & 5.7s &  24.4s &  23.2s & 19.8s & 18.3s & 14.8s \\
Compression preprocess & N/A & 01h 09m 37s & 01h 08m 25s & 01h 26m 57s & 02h 18m 45s & 02h 11m 51s \\
\hline
\end{tabular}
\vspace*{-0.25em}
\caption{\em Compression performance for each of the three scenes. The ``sweet spot'' for each scene that achieves a good balance between compression and visual quality is shown in gray.}
\label{table:timing-comparisons}
\end{center}
\end{table*}

%%%%%%%%%%%%%%%%%%
%% ADJ: new table for laptop data.
%\begin{table}
%\begin{center}
%\begin{tabular}{|c||c|c|} \hline
% & Time per frame & Speedup \\
%\hline
%Full space & 286.5s & N/A \\
%\hline
%Uncompressed &  N/A & N/A \\
%\hline
%14 : 1 compression & 71.7s & 4.0X \\
%\hline
%\end{tabular}
%\end{center}
%\caption{\label{tab:thrashing}Timings of the sphere simulation running on a 2 core, 8 GB RAM laptop. The $14 : 1$ compression makes the 70+ GB memory footprint tractable, reducing it to about 5.3 GB. The uncompressed simulation thrashes the virtual memory after approximately 4 hours and cannot feasibly run the scene.}
%\end{table}
%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
% In GIMP, using font Palatino of font size 50.
\includegraphics[width=\columnwidth]{chap4/figures/plumeComparison_brighter.png}
\caption{\textbf{Plume scene:} {\em The overall motion and visual quality of the plume is preserved until the compression ratio is increased to approximately $22:1$. The $1:1$ corresponds to using the original $\UU$ matrix.}}
\label{fig:plumeComparison}
\end{figure}

\noindent \textbf{Plume scene:} We simulated the buoyant flow of a plume in a scene containing no obstacles, as shown in Figure \ref{fig:plumeComparison}. The original simulation resolution is $200 \times 266 \times 200$, was run for 150 frames, and took 09h 48m 49s (3.92 minutes per frame).

The SVD to construct the subspace from this data took 07h 05m 40s, and the compressing the subspace took at most 02h 15m 59s (Table \ref{table:timing-comparisons}). Constructing and compressing the subspace is therefore roughly at parity with running the entire simulation a second time. However, once the subspace has been constructed once, we can run new simulations very quickly.

We found that the subspace could be compressed by roughly an order of magnitude ($11:1$) before the visual quality began to degrade. However, we also noted that the compression scheme appears to degrade relatively gracefully. For higher compression rates, the motion gradually deviates from the uncompressed motion, and JPEG-like block artifacts begin to appear (cf. the actual JPEG artifacts exhibited in \S\ref{sec:jpeg}).

\begin{figure}[t]
\includegraphics[width=\columnwidth]{chap4/figures/spheres_sidebyside_14.png}
\caption{\textbf{Sphere scene:} {\em The motion and visual quality remains high until approximately $14:1$ compression. At $30:1$, the differences are very significant.}}
\label{fig:sphereComparison}
\end{figure}

\noindent \textbf{Sphere scene:} Next, we simulated the same plume in the presence of a sphere obstacle. The original simulation resolution is $200 \times 266 \times 200$, was run for 150 frames, and took 10h 37m 50s (4.25 minutes per frame). The time to construct and compress the subspace, respectively 09h 17m 19s and a maximum of 02h 34m 30s, was again found to be roughly at parity with running the simulation a second time. 

Surprisingly, we found that this subspace compressed slightly better than the plume scene ($14:1$) before the visual quality began to degrade. The expectation was that the sphere boundary would create a discontinuity in the velocity field that the compressor would have trouble capturing. Instead, the interior of the static obstacle created a region of constant (zero) velocity, and also induced the formation of smooth, near-zero regions in its vicinity. Rather than create a discontinuity containing many high frequencies, these constant and smooth regions mostly contained low-frequencies that the compressor could easily leverage. No-slip boundaries were used along the surface of the obstacle; if free-slip were used instead, the anticipated discontinuities may still appear.

For this scene, we also compared our adaptive quantization strategy to a uniform, non-adaptive approach. There is no canonical 3D version of Eqn.~\ref{eq:q50}, so we instead selected a uniform $\gamma$ that produced an equivalent energy loss in the highest frequency component, i.e.,~we set $\gamma$ such that it matched the 32-bit equivalent of the lower-right hand entry of Eqn.~\ref{eq:q50}. This strategy still produced a $7:1$ compression, but our adaptive strategy was able to achieve a higher compression of $14:1$.

\noindent \textbf{Out-of-core Comparison:} We compared our performance to an uncompressed simulation that does not fit in core by running the $14 : 1$ version of the sphere scene on a 2-core, 1.8 GHz Macbook Air with 8 GB RAM. On this system, the full space simulation ran at 286.5s per frame, while our compressed subspace simulation ran at 71.7s, yielding a speedup of roughly $4.0\times$.

The uncompressed subspace simulation requires over 70 GB of memory, and immediately started to swap on the 8 GB system. It ran for over 17 hours without completing a single frame, at which time it had more than doubled the running time of the original full space simulation, and was terminated.

\begin{figure*}[t]
\includegraphics[width= \columnwidth]{chap4/figures/fan_sidebyside.png}
\caption{\textbf{Fan scene:} {\em The quality is preserved at $6:1$, but at $11:1$, the motion begins to change, and at $29:1$, artifacts begin to appear.}}
\label{fig:fanComparison}
\end{figure*}
\noindent \textbf{Fan scene:} Finally, we simulated smoke being stirred by a fan. The original simulation resolution is $266 \times 200 \times 200$, was run for 150 frames, and took 12h 18m 05s (4.92 minutes per frame). The subspace construction and compression times were 08h 13m 22s and at most 02h 18m 45s.

The moving obstacle scene achieved a compression ratio of $6:1$ before the quality began to degrade. Here, the reduced compression we expected to see in the sphere scene appeared. Instead of having a smoothing effect on the velocity field, the obstacle creates new, high-frequency velocities that are more difficult to compress. However, we did not choose to leverage any knowledge of the obstacle motion during compression, which could be a promising avenue for finding sparser representations.

%\begin{figure}[h]
%\includegraphics[width=\columnwidth]{Figures/plume_resim.png}
%\caption{\textbf{Plume Re-Simulation:} With vorticity set to zero, the overall quality of the subspace re-simulation remains intact.}
%\label{fig:plumeResim}
%\end{figure}

\section{Discussion and Conclusions}

In order to determine whether the compression compromised the generality of the subspace, we also ran re-simulations \cite{Kim2013} on each of our scenes using a variety of different simulation settings. In the plume scene, we both reduced the vorticity confinement constant to zero and doubled the number of total timesteps (see video), for the sphere scene we halved the buoyancy constant (see video) and in the fan scene, we increased the vorticity confinement constant by a factor of ten (Fig.~\ref{fig:fanResim}). The re-simulations were all run on the ``sweet-spot'' compression ratios that are highlighted in Table \ref{table:timing-comparisons}. In all cases, the overall motion was preserved, and we did not observe any significant visual artifacts compared to the uncompressed subspace re-simulation.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{chap4/figures/paddle_resim.png}
\vspace*{-1em}
\caption{\textbf{Fan Re-Simulation:} {\em With vorticity confinement increased by a factor of ten, the novel turbulent detail that is introduced remains intact, even after basis compression.}}
\label{fig:fanResim}
\end{figure}

\begin{figure*}
\centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{chap4/figures/plumeError}
%    \caption{Comparison of compression errors for plume.}
    \label{fig:plumeError}
  \end{subfigure}
  %
   \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{chap4/figures/sphereError}
 %   \caption{Comparison of compression errors for sphere.}
    \label{fig:sphereError}
    \end{subfigure}
    %
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{chap4/figures/fanError}
 %   \caption{Comparison of compression errors for fan.}
    \label{fig:fanError}
  \end{subfigure}
 % \vspace{-3em}
 \caption{\em Relative $L_2$ error introduced by the compressed  matrix $\CC$ compared to an uncompressed $\UU$ over the course of a simulation. The transition between visually undetectable and visible error corresponds to roughly an order of magnitude jump in the relative error.}
\end{figure*}

In order to better understand the error introduced by the compression, we plotted the relative $L_2$ error between the $\utilde$ vectors obtained by the uncompressed and compressed simulations. While it is difficult to tell whether a given compressed $\UU$ is too aggressive {\em a priori}, an overaggressively-compressed $\UU$ quantitatively corresponds to a $10^{-1}$ relative error appearing early in the simulation. In this case, the compression error exceeds that of the cubature integration scheme \cite{Kim2013}, and begins to visually dominate.

\noindent \textbf{Conclusions and Future Work:} We have presented a compression method for subspace fluid simulations that is able to reduce the size of the $\UU$ matrix by up to an order of magnitude. The most immediate direction for future work is the development of algorithms that are able to reduce this matrix size even further. As seen in Table \ref{table:timing-comparisons}, as the sparsity of the compressed representation increases, the speed of the matrix-vector product improves. According to our asymptotic analysis, it should eventually surpass the performance of the direct $\UU\utilde$ product. Therefore, finding new bases that possess the same unitary property as the DCT while increasing sparsity should yield algorithms that are efficient in both memory and time.

Lossless compression schemes such as Huffman coding could also be investigated, but these would not directly increase the sparsity of the representation, so the tradeoff between memory savings and decompression speed would need to be balanced. Finally, the method we have presented assumes that the velocity field is defined on a regular grid. An approach that can be applied to unstructured, tetrahedral meshes \cite{Liu:2015:MVF,Stanton:2013} would also be welcome.
















