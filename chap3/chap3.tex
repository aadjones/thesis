\chapter[Background]{Background}
\label{chap:chap3}
\section{Fluids}

A {\em fluid} is a substance that continuously deforms under the application of shear stress \cite{batchelor2000introduction}. More intuitively, it is a substance that {\em flows}. Although colloquially we may use the term fluid to refer only to liquids, gases can exhibit flow, and are therefore considered to be fluids as well. (Indeed, the results demonstrated in this dissertation focus entirely on the turbulent flow of smoke.) The natural world is full of captivating fluid motions: ocean waves, ripples in a pond, clouds, the steam of a teakettle, etc. These flowing patterns, while beautiful, appear difficult to formalize precisely, and the modeling of fluids continues to be a mathematical and computational challenge to this day.

\section{Physics-based modeling}
\label{sec:physics-based}
The field of {\em fluid dynamics} in physics seeks to model and describe fluid flow systematically according to equations generated from a set of basic assumptions and principles. In order for continuous mathematical operators to be used to describe fluids, one of the basic assumptions is that of the {\em continuum}: for mathematical purposes, we assume that fluid velocity varies continuously across space, even though in reality, fluids are actually composed of discrete molecules. Besides this continuum assumption, the various conservation laws of mass, energy, and momentum, imply the famous Navier-Stokes equations \cite{currie2012fundamental}, which describe the velocity field $\uu$ of fluid flow depending on the intrinsic stress of viscosity and pressure. In the case where we make the additional simplifying assumption that the fluid is incompressible,\footnote{i.e., the fluid density does not vary within the flow field} the equations take the following form:

\begin{equation}
\label{eq:incompress}
\nabla \cdot \uu = 0
\end{equation}
\begin{equation}
\label{eq:momentum}
\frac{\partial \uu}{\partial t} = -\left(\uu \cdot \nabla \right)\uu + \nu \nabla^{2}\uu - \nabla{p} + \mathbf{f}_e
\end{equation}

The incompressibility assumption is violated for fluid dynamics such as supersonic flows; however, these types of motions are not the dynamics we seek to model in this dissertation. There are several terms here that must be unpacked:

\begin{itemize}
\item The time-varying vector field $\uu = u(\xx, t)$ describes the fluid velocity field. It is the main quantity we are interested in, and solving these differential equations amounts to describing what $\uu$ is for all times $t$.
\item The constant $\nu$ represents the viscosity of the fluid, which represents its resistance to deform while flowing. (Intuitively, this can be thought of its resistance to being stirred: molasses, for instance, has higher viscosity than water.) 
\item The time-varying scalar field $p = p(\xx, t)\ $ represents the pressure field of the fluid. 
\item The time-varying vector field $\mathbf{f}_e = \mathbf{f}_e(\xx, t)$ represents any external forces affecting the fluid, such as gravity.
\end{itemize}
Equation \ref{eq:incompress} enforces the assumption of incompressibility, while equation \ref{eq:momentum} is a consequence of the aforementioned conservation laws.

\section{Simulation}

Simulation is the imitation of a natural process, in this case over time. Any simulation must first determine a technique of modeling the process computationally, and there are often many different possible strategies. We consider here a few typical approaches in computational fluid dynamics.

As in all numerical techniques of modeling fluids, we first discretize both the spatial and time domain. In other words, to perform the calculations, we dice up the region of space into a regular grid of small cubical cells. Each cell then contains a vector, or arrow, describing the velocity of the fluid at the position in space. In order to evolve the motion of the flow over time, we also dice up time itself into a sequence of discrete time steps, computing how the velocity of the fluid in each cell changes at each time step. This approach is called the {\em Eulerian} viewpoint of the flow. Intuitively, this can be thought of as staying in a fixed location and observing the flow through that particular location. More precisely, the fluid's velocity in this viewpoint is represented as $\uu(\xx, t)$, which gives the velocity of the fluid at each spatial location $\xx$ and time $t$.

In contrast, the {\em Lagrangian} viewpoint of the flow takes the point of view of each individual fluid particle as it moves along the flow through space and time. Intuitively, this can be thought of drifting along with the flow. If we label each particle based on its position in space using a vector field $\mathbf{r}$, we then describe the fluid flow with the position field $\mathbf{X}(\mathbf{r}, t)$ , which gives the position of each particle labeled by $\mathbf{r}$ at time $t$.

Following \cite{lamb1932hydrodynamics}, we can connect these two viewpoints with the following equation:
\begin{equation}
\label{eq:euler-lagrange}
\uu\left(\mathbf{X}\left(\mathbf{r}, t\right)\right) = \frac{\partial \mathbf{X}}{\partial t}\left(\mathbf{r}, t\right)
\end{equation}

The left-hand side of equation \ref{eq:euler-lagrange} describes the velocity of the flow at position $\mathbf{r}$ and time $t$ using the Eulerian-specified velocity field $\uu$, while the right-hand side describes the same velocity by taking the partial derivative of the Lagrangian specified position field $\mathbf{X}$.

Given an Eulerian-specified fluid velocity field $\uu(\xx, t)$ over a domain of space, and given another Eulerian-specified scalar field $\varphi(\xx, t)$ (e.g., temperature) over the same domain, we can consider the total derivative of $\varphi$. By the chain rule, this expands as follows:

\begin{equation}
\frac{\mathrm{d}}{\mathrm{d}t}\varphi\left(\xx, t\right) = \frac{\partial \varphi}{\partial t} + \dot{\xx} \cdot \nabla \varphi
\end{equation}

The partial derivative term $\frac{\partial \varphi}{\partial t}$ indicates the instantaneous rate at which the temperature is changing with time, holding the spatial position constant. Thus, if we imagine a fluid flow being comprised of many infinitesimal particles, this term represents any change in temperature of a particle at a fixed spatial location. The remaining term $\dot{\xx} \cdot \nabla \varphi$ describes a more dynamic rate of change, as it incorporates changes in temperature caused by moving {\em along} the flow through the path $\dot{\xx}$. This movement is in the Lagrangian spirit of drifting along with the flow. In the case where $\dot{x}$ is taken to align with the flow velocity $\uu$, we have 

\begin{equation}
\frac{\mathrm{d}}{\mathrm{d}t}\varphi\left(\xx, t\right) = \frac{\partial \varphi}{\partial t} + \uu \cdot \nabla \varphi
\end{equation}
which is often called the {\em material derivative}. The operator $\uu \cdot \nabla$ is the advection operator, which we will be prominent later on.

\subsection{Numerical Simulation}
Numerical simulation brings the abstract equations of mathematical simulation to a concrete, computational implementation. Typically, this involves discretization of any continuous parameters in the model, such as time and 
space. As such, idealized concepts such as spatial and time-dependent derivatives must be approximated. Numerical analysis is a broad and rich field in applied mathematics and engineering, and many different approaches
are available, each with various pluses and minuses. For the scope of this dissertation, we consider the numerical method of Jos Stam's {\em Stable Fluids} \cite{Stam99}, which is the backbone of our fluid simulation technique. 

The first step in Stam's method, following the technique of Chorin and Marsden in \cite{chorin1990mathematical}, is to devise a technique to combine the two equations \ref{eq:incompress} and \ref{eq:momentum} into one equation. We rely on a result from vector calculus called the Helmholtz-Hodge decomposition, which states that any arbitrary vector field $\ww$ can be decomposed uniquely into the sum of a divergence-free, or solenoidal, vector field and a curl-free, or irrotational, vector field. More precisely,
we can write

\begin{align}
\label{eq:hh}
\ww &= \uu_{\text{solenoidal}} + \uu_{\text{irrotational}}\\
        &= \uu_{\text{solenoidal}} + \nabla q
\end{align}
since the curl $\nabla \times \nabla q$ must be zero for some scalar field $q$. Hence, we can define a projection operator $P$ which maps a vector field to its (unique) solenoidal component. In other words,
$P \ww = \uu_{\text{solenoidal}}$. Indeed, this operator can be discovered explicitly by taking the divergence of both sides:

\begin{equation}
\nabla \cdot \ww = \nabla^2 q
\end{equation}
which is a Poisson equation for the scalar field $q$. Given $\ww$, we can solve this equation to obtain $q$, and then compute $\uu_{\text{solenoidal}}$ via $\uu_{\text{solenoidal}} = \ww - \nabla q$. Thus, we can take the
second equation in the Navier-Stokes equations, \ref{eq:momentum}, and apply the projection operator $P$ to both sides, obtaining

\begin{equation}
\label{eq:stamns}
\frac{\partial \uu}{\partial t} = P\left(-\left(\uu \cdot \nabla \right)\uu + \nu \nabla^{2}\uu + \mathbf{f}_e\right)
\end{equation}
This conveniently eliminates the pressure term $\nabla p$ and unites the two equations \ref{eq:incompress} and \ref{eq:momentum} into one.

The next important technique that Stam's method uses involves {\em splitting} the operators in the Navier-Stokes equations \ref{eq:stamns} into a sequence of simpler operators. In other words, because the right-
hand-side of \ref{eq:stamns} can be regarded as a combination of several terms (namely, an advection term, a diffusion term, an external forces term, and a projection) \ref{eq:splitting}, we focus our attention on solving each of these simpler differential equations in isolation. Once we have a solution to each individual term, we can construct an approximation for the solution to the more complex full equation \ref{eq:stamns}. Establishing some notation, suppose we begin from an initial state $\uu_0 = \uu(\xx, 0)$ and would like to step forward in time by a timestep $\Delta t$. Beginning from the previous timestep's solution $\ww_0 = \uu(\xx, t)$, we solve a sequence of equations $\ww_0, \dots, \ww_4$, ending with the last velocity field which is equal to the value at the next time step: $\ww_4(\xx) = \uu(x, t + \Delta t)$. Iterating this procedure generates a complete numerical solution, as desired.

\begin{equation}
\label{eq:splitting}
\ww_0(\xx) \xrightarrow{\text{add forces}} \ww_1(\xx) \xrightarrow{\text{advect}} \ww_2(\xx) \xrightarrow{\text{diffuse}} \ww_3(\xx) \xrightarrow{\text{project}} \ww_4(\xx)
\end{equation}

The external forces can be approximated simply using forward Euler or other explicit schemes. In other words, we can write
$\ww_1(\xx) = \ww_0(\xx) + \Delta t \mathbf{f}_{\text{ext}}(\xx, t)$. The advection term, however, is nonlinear and adds considerable complexity to the solution. Stam's insight was to consider the advection from a Lagrangian viewpoint. Intuitively speaking, each fluid particle is advected by its own velocity. Hence, to calculate the velocity at a particular spatial point $\xx$ at the next time step $t + \Delta t$, we backtrace from the point $\xx$ through the previous velocity field over a time step of $\Delta t$. Letting $\xx$ vary, following Stam's notation, this defines a path $\mathbf{p}(\xx, s)$ along a partial streamline of the velocity field. We assume then that the new velocity at the point $\xx$ at time $t + \Delta t$ must therefore be the same as the old velocity at the backtraced point at time $\Delta t$ previous. (Although the backtraced point may not lie exactly on a grid cell, we can use linear interpolation to reslve this problem.) More concretely, we have $\ww_2(\xx) = \text{Interpolate}\left(\ww_1\left(\mathbf{p}(\xx, -\Delta t)\right)\right)$.
Because this approach exploit both the Eulerian and Lagrangian viewpoints, it is known as a {\em semi-Lagrangian} strategy.

One of the key advantages to semi-Lagrangian advection technique is that it is {\em unconditionally stable}: in other words, no matter how large a time step $\Delta t$ we select, the velocity will remain bounded. Intuitively, this is straightforward to observe: each velocity update is the result of an interpolation of previous velocity values, and therefore cannot be any greater than any of the previous values. From a practical standpoint, this numerical stability is a big plus, and the heart of Stam's argument as to
why this method is useful in computer graphics. However, with very large time steps, the problem of numerical dissipation becomes more of an issue. A more detailed discussion of these issues can be found in \cite{bridson2015fluid}.

The remaining terms in the splitting are the diffusion and pressure projection. The diffusion term is simply the heat equation, relying on the Laplace operator:

\begin{equation}
\frac{\partial{\ww_2}}{\partial{t}} = \nu \nabla^2 \ww_2
\end{equation}

Many well-known techniques exist for numerical solutions to this equation, including discretization of the Laplace operator as well as implicit methods \cite{peaceman1955numerical}.

Finally, as observed previously, the pressure projection reduces to a Poisson equation to solve for $q$ from $\ww_3$.

\begin{equation}
\nabla^2 q = \nabla \cdot \ww_3
\end{equation}

From here, we can recover $\ww_4$ via the equation $\ww_4 = \ww_3 - \nabla q$. Hence, the numerical splitting is complete.

Within Stam's paper is another interesting topic of carrying out the equations of motion within the Fourier domian, explored further in \cite{stam2001simple}. Although 
we do not implement our solver using this technique, it is a topic of interest based on our compression technique as well as,
our frequency-domain interpretation of the fluid modes and our corresponding sonification technique later on, so we delve into a few details here. The key observation is that the Fourier transform diagonalizes spatial derivatives. In other words, if we denote the Fourier transform of $\uu(\xx)$ by $\fancyF[\uu(\xx)](\kk) = \uhat(\kk)$, then we have $\fancyF[\nabla \uu(\xx)](\kk) = i \kk \uhat(\kk)$. The divergence operator similarly satisfies $\fancyF[\nabla \cdot \uu(\xx)](\kk) = i \kk \cdot \uhat(\kk)$, and the Laplacian, being the divergence of the gradient, satisfies $\fancyF[\nabla^2 \uu(\xx)](\kk) = - k^2 \uhat(\kk)$, where $k = |\kk|$. The upshot of all this is that we can view some of the fluid operators in the spatial domain in simple ways in the frequency domain. For example, the diffusion, based on the Laplacian operator, can be thought of as a low-pass filter with decay governed by the viscosity. In general, this mode of
thinking bears fruit later for our discussions of the compression algorithm in Chapter \ref{chap:chap4} and our sonification system in Chapter \ref{chap:chap5}.

\section{Subspace Methods}
\label{sec:subspace}
Subspace methods, also known as model reduction, or reduced order methods, has a long history in engineering and applied mathematics, including applications to fluid simulation \cite{lumley1967}, and was introduced to the computer graphics literature in 1989 by Pentland and Williams \cite{Pentland:1989:GVM, Berkooz93theproper}. The basic principle of these methods, as the name suggests, is to reduce the computational complexity of a large numerical simulation. A typical simulation will have many degrees of freedom in principle, generating a vast state space of possibilities. However, in practice, not all of these degrees of freedom are of equal importance. By systematically discovering a reduced subspace of the state space, and carrying out calculations within this subspace, a subspace simulation can lead to large computational accelerations with minimal degradation of accuracy.

\subsection{Modal Analysis}
\label{sec:modal}
Following Barbi{\v{c} and James \cite{barbivc2005real}, to see an example of the subspace method in action, consider the modeling of small deformations of rigid bodies. Although rigid bodies in principle have many degrees of freedom over which they can deform, they have certain characteristic shapes into which they are more likely to deform. These shapes, also known as {\em modes}, are the ones which minimize the strain, and in an intuitive sense are the ``natural'' deformations of the rigid body. To compute these, we assume the rigid body is discretized into a mesh with $N$ vertices. We then compute the system mass and stiffness matrices $\MM \in \R^{3N \times 3N}$ and $\KK \in \R^{3N \times 3N}$, respectively. The modes then satisfy the generalized eigenvalue equation

\begin{equation}
	\KK \xx = \lambda \MM \xx
\end{equation}

Due to the special properties of the matrices $\MM$ and $\KK$, which are both symmetric positive-definite (SPD), the eigenvalues $\lambda_i$ are positive real numbers. 
Although there are $3N$ eigenvalues in total, in practice, we can take a subset of them, $\lambda_1, \lambda_2, \dots, \lambda_r$, from least to greatest, and their associated eigenvectors
 $\psi_1, \psi_2, \dots, \psi_r$. (Here, $r \ll 3N$ represents the number of modes we wish to retain.) The eigenvectors $\psi_i$ we choose corresponding to the eigenvalues $\lambda_i$ 
 are not unique; for convenience, we select those that satisfy the mass-orthogonality condition $\langle \MM \psi_i, \psi_j \rangle = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta, 
 equaling $0$ for $i \neq j$ and $1$ for $i=j$. The $r$-dimensional subspace $S \subset \R^{3N}$ formed by the linear span of these eigenvectors can be encapsulated in matrix form 
 by assembling the modal basis matrix $\UU \in \R^{3N \times r}$ as follows:

\begin{align}
	\UU^{T} &= \begin{pmatrix}
	\vertbar & \vertbar &   & \vertbar \\
	\psi_1 & \psi_2 & \dots & \psi_r   \\
	\vertbar & \vertbar &   & \vertbar
  \end{pmatrix}
\end{align}

Thus, the mass-orthogonality condition we specified for the modes can be captured in the more succinct matrix form $\UU^{T} \MM \UU = \II_r$,
where $\II_r \in \R^{r \times r}$ denotes the $r \times r$ identity matrix.

Now consider the typical equation of motion for undamped small deformations $\xx \in \R^{3N}$ corresponding to external forces $\ff \in \R^{3N}$: 
\begin{equation}
\label{eq:mass-spring}
\MM \ddot{\xx} + \KK \xx = \ff
\end{equation}

Equation \ref{eq:mass-spring}, is a very high-dimensional linear differential equation obtained by applying the standard equations of elastic deformation to the mesh with $3N$ nodes using the finite element method (FEM). For very fine meshes with large values of $N$, this equation may present a computational bottleneck. Hence, the approach of model reduction is welcome. We proceed by approximating $\xx$ as a linear combination of the modes $\psi_1, \dots, \psi_r$:

\begin{equation}
\xx \approx \UU \qq
\end{equation}
where the vector $\qq \in \R^{r}$ has components $q_i$ that are the corresponding weights of the associated modes $\psi_i$, $i=1,\dots,r$. This vector $\qq$ is called the {\em reduced coordinates} of $\xx$, and can also be thought of as the projection of $\xx$ into the subspace $S$. Rewriting equation \ref{eq:mass-spring}, we obtain

\begin{equation}
\MM {\UU \ddot{\qq}} + \KK \UU \qq = \ff
\end{equation}
By the generalized eigenvector condition, we have $\KK \UU = \Lambda \MM \UU$, where $\Lambda = \textnormal{diag}\left(\lambda_1, \dots, \lambda_r\right)$. Hence, we simplify as follows:

\begin{equation}
\MM \UU \left(\ddot{\qq} + \Lambda \qq\right) = \ff
\end{equation}
Next, to clear away the factor of $\MM \UU$, we pre-multiply both sides by $\UU^{T}$:

\begin{equation}
\UU^{T} \MM \UU \left(\ddot{\qq} + \Lambda \qq\right)= \UU^{T} \ff
\end{equation}
By the mass-orthogonality condition, the term $\UU^{T} \MM \UU$ reduces to $\II_r$.  On the right-hand-side, $\widetilde{\ff} = \UU^{T} \ff \in \R^r$ is the reduced form of $\ff$. Hence, we have the following lower-dimensional linear differential equation with unknown $\qq \in \R^{r}$:

\begin{equation}
\ddot{q} + \Lambda \qq = \widetilde{\ff}
\end{equation}
In this particular case, the system is uncoupled since $\Lambda$ is diagonal, so we have a collection of $r$ independent one-dimensional ordinary differential equations. These can be integrated forward in time very efficiently, obtaining $\qq_{t+1}$, the value of $\qq$ at the next time step. To finish the computation, we reconstruct into the full-dimensional space by computing $\xx_{t+1} = \UU \qq_{t+1}$.

\subsection{Sound generation}
Small rigid deformations of a solid body are precisely those that generate sound waves. Hence, following O'Brien \cite{O'Brien:2001:SSP:945191.945250}, we briefly consider the application to audio of the previous discussion. Recall that our subspace reduction produces a collection of $r$ independent one-dimensional equations, which, expanded out, take the form

\begin{equation}
\label{eq:system}
\ddot{q_i} + \lambda_i q_i = \psi_i^{T} f_i, \ \ i = 1, \dots, r
\end{equation}

Recalling that each $\lambda_i$ is a positive real number, let $\omega_i$ be the positive square root $\omega_i = \sqrt{\lambda_i}$. Then the general analytical solution to the homogenous portion of each one-dimensional differential equation in \ref{eq:system} is given by $q_i = c_1 e^{i \omega_i t} + c_2 e^{i \omega_i t}$. In other words, we can think of the $i$-th mode as resonating at a natural frequency of $\omega_i$. This allows us to perform simple additive synthesis by mixing together a collection of sinusoids at the corresponding frequencies, generating the audio signal that corresponds to the physical deformation. The spirit of this basic approach motivates
our sonification process in Chapter \ref{chap:chap5}.

\subsection{A more general subspace formulation}
\label{sec:general-subspace}
We have seen how subspace methods apply to rigid deformations, which can be characterized by linear systems. However, more general and complex phenomena, including fluid flow, require non-linear characterizations. Hence, we consider a more general framework for applying subspace methods, following Treuille 
\cite{Treuille:2006:MRF}.

In the most abstract setting, the subspace technique takes a vector $\uu \in \R^N$ living in a high-dimensional space and maps it to a corresponding reduced-space vector $\qq \in \R^r$, with $r \ll N$. (We refer to this high-dimensional space
as the ``full space'' and the reduced space as the ``subspace.'') This mapping can be seen as a projection operator $P : \R^N \rightarrow \R^r$, which carries $\uu$ to $\qq$, and its corresponding reconstruction operator $P^{-1} : \R^r \rightarrow \R^N$, which lifts $\qq$ back up to $\uu$. (Note that since $r < N$, the projection map $P$ is not one-to-one, and hence necessarily must lose some information.) 

Within physical simulations, we are not concerned just with the projection mapping of one vector in the high space down to its corresponding reduced vector, but also a framework for time evolution. In other words, given a differential equation in the full space

\begin{equation}
\dot{\uu} = F(\uu, t)
\end{equation}
we would like to formulate a corresponding reduced-space differential equation

\begin{equation}
\dot{\qq} = \hat{F}(\qq, t)
\end{equation}
so that the equations of motion can be integrated through time within the reduced space. In particular, we desire a reduced-space differential equation 
that can be solved in asymptotic time depending only on the reduced dimension $r$ and not the full dimension $N$.

The obvious and standard approach is to compute $\hat{F}$ via what is known as {\em Galerkin projection} as follows:

\begin{equation}
\hat{F} = P \circ F \circ P^{-1}
\end{equation}
In the situation of linear equations, this projection as described before can be described with matrix-vector multiplies. In particular, the matrix corresponding
to $\hat{F}$ can be precomputed, leading to large speedups. However, nonlinear dynamics must be handled with more sophisticated methods. Furthermore, in the absence of analytical eigenmodes, as utilized during section \S \ref{sec:modal}, it is unclear how to select a set of basis vectors to form a reasonable reduced space $\R^r$ in the first place. 

Treuille's technique is to form a representative basis using ``snapshots'' from a corresponding full-space fluid simulation. Ideally, these snapshots capture the 
user's desired range of motion and dynamics. Upon performing a form of Principal Component Analysis (PCA), a set of basis vectors $\qq_1, \dots, \qq_r$ are derived. Following the basic strategy of splitting the Navier-Stokes equations, it remains then to demonstrate how to perform reduced-space external forces,
advection, diffusion, and projection. While most of these operations can be viewed as a full-space matrix, and thus projected and precomputed, the advection
term, being nonlinear, requires a more careful approach. Although in principle, by discretizing the advection operator, a matrix corresponding
to the advection term could be formulated, this matrix will depend on the current timestep, and thus cannot be precomputed like the others. To combat this issue,
the original approach of Treuille is to construct a static third order tensor, which can be precomputed. However, as Kim and Delaney point out, this technique
will not in general be a {\em consistent} integration method \cite{Kim2013, Carlberg11}. The intuition for this problem is that the finite difference methods in the reduced space are not equivalent to the semi-Lagrangian methods in the full-space, but further details are beyond the scope of this dissertation. However, the need for a different strategy for advection and other nonlinear phenomena motivates the notion of cubature schemes, described next.

\subsection{Cubature schemes}
The work of An et al. \cite{An:2008} brought the idea of cubature methods to nonlinear solid mechanics in computer graphics. The term cubature itself is a multidimensional analogue
of the classical one-dimensional problem of {\em quadrature}, which seeks to approximate the definite integral of a function by estimating it as a weighted 
sum of point-sampled versions of the function. More concretely, we have an approximation of the form

\begin{equation}
\int_a^b f(x) dx \approx \sum_{i=1}^n w_i f(x_i)
\end{equation}
where the $w_i$ are weights and the $x_i$ are the point samples. Careful choice of these weights and point samples leads to efficient and accurate estimation of the integral.

In the more general setting, following Kim and Delaney \cite{Kim2013}, we introduce some standard notation. Suppose our full space is $\R^{3N}$ and our subspace is $\R^{r}$, so that our subspace 
projection matrix is $\UU \in \R^{3N} \times r$. Furthermore, we denote reduced-space quantities with an over-tilde, e.g., $\tilde{\uu} = \UU^T u \in \R^{r}$ denotes the projection by $\UU$ of $\uu$ in the full space 
down to $\tilde{\uu}$ in the reduced space. Next, suppose we have a (possibly nonlinear) function $\fancyF : \R^3 \rightarrow \R^3$ and a vector of $N$ points in $\R^3$, $\xx \in \R^{3N}$ that represents
a velocity field on a grid. Since $\fancyF$ can operate on any point $p \in \R^3$, we can transform the entire vector $\xx$ pointwise by having $\fancyF$ operate on each of its points, at each stage computing $\ff_p = \fancyF_p(\xx_p) \in \R^3$. This
yields a corresponding vector $\mathbf{f} \in \R^{3N}$ that represents $\xx$ pointwise transformed by $\fancyF$. (Although the discussion here is generic to what $\fancyF$ represents, to give a concrete
example, $\xx$ could be a velocity field prior to advection, $\fancyF$ could be an advection scheme, and $\mathbf{f}$ could be the resulting velocity field after advection.)

In the full space, we have a mapping from $\xx$ to $\mathbf{f}$ via $\fancyF$, so in order for the subspace technique to work, we need to discover a mapping from $\tilde{\xx}$ to $\tilde{\mathbf{f}}$. 
The simplest cases are when $\fancyF$ itself is a linear transformation, so that its projection can be precomputed, but in the nonlinear case, as discussed previously, we need another strategy.
Observe first that if we are willing to calculate with full-space coordinates, we can write

\begin{equation}
\label{eq:fulladvect}
\tilde{\mathbf{f}} = \UU^T \mathbf{f} = \UU^T \fancyF \xx = \UU^T \fancyF(\UU\tilde{\xx})
\end{equation}
which implies a method from computing $\tilde{\mathbf{f}}$ starting from $\tilde{\xx}$ by reconstructing first into the full space via $\UU$, applying $\fancyF$, and then projecting back
down into the subspace. This method, however, is counter to the spirit of the subspace approach, as it depends on the dimension of the full space, $N$. The insight of Kim and Delaney is to regard
Equation \ref{eq:fulladvect} as a multidimensional integral over the entire simulation domain $\Omega$:

\begin{equation}
\label{eq:integral}
\tilde{\mathbf{f}} = \UU^T \fancyF(\UU\tilde{\xx}) = \int_{\Omega} \UU_p^T \fancyF_p(\UU_p \tilde{\xx})d\Omega
\end{equation}

Now in integral, we can therefore apply the method of cubature \cite{Press:1992:NumRecipes} to obtain an efficient approximation of $\tilde{\mathbf{f}}$. In particular, we would like to select a set
$\mathcal{S}$ of points $p \in \mathcal{S}$ and corresponding weights $w_p \in \R$ so that we have

\begin{equation}
\label{eq:integral}
\tilde{\mathbf{f}} = \int_{\Omega} \UU_p^T \fancyF(\UU_p \tilde{\xx})d\Omega \approx \sum_{p \in \mathcal{S}} w_p \UU_p^T \fancyF(\UU_p \tilde{\xx})
\end{equation}
Provided that the cardinality $|\mathcal{S}|$ is on the order of the reduced space-dimension $r$ rather than the full-space dimension $N$, this approach will pay dividends. However, it remains to see
how to select both the cubature set $\mathcal{S}$ and the weights $w_p$. 

The original approach by An is to select a set of cubature points and corresponding weights that minimize the error of a training set. First, we describe the method of choosing weights,
given a particular set of cubature points. To that end, suppose we have a collection of $T$ snapshots, $\ff^1, \dots, \ff^T$, and
their associated reduced-space projections $\tildef^1, \dots, \tildef^T$, where $\tildef^i = \UU^T\ff^i$. In addition, suppose we have selected a collection of $n$ cubature points. Then we look to solve the
following nonnegative least squares (NNLS) problem:

\begin{equation}
\label{eq:cubature}
\begin{bmatrix}
    \tildef_{1}^1       & \cdots & \tildef_{i}^{1} & \cdots & \tildef_{n}^1\\
    \vdots                 &              & \vdots            &             & \vdots\\
    \tildef_{1}^{t}     & \cdots  & \tildef_{i}^{t} & \cdots  & \tildef_{n}^t\\
    \vdots                 &              & \vdots            &             & \vdots\\
    \tildef_{1}^T      & \cdots   & \tildef_{n}^T & \cdots & \tildef_n^T
\end{bmatrix}
\begin{bmatrix}
	w_1\\
	\vdots\\
	w_n
\end{bmatrix}
=
\begin{bmatrix}
	\tildef^1\\
	\vdots\\
	\tildef^t\\
	\vdots\\
	\tildef^T
\end{bmatrix}
\end{equation}
with the unknown weight vector $\mathbf{w}$ satisfying $\mathbf{w} \geq \mathbf{0}$. For convenience,
we abbreviate equation \ref{eq:cubature} as $\boldA \ww = \bb$, with $\boldA \in \R^{rT \times n}$, $\ww \in \R^n$, and $\bb \in \R^{rT}$. Each column of the matrix $\boldA$ represents subspace point-sampled versions 
of the advection operator of the snapshot $t \in [1 \dots T]$ at the corresponding cubature point $i \in [1 \dots n]$. The vector $\ww$ represents the unknown nonnegative weights we seek. Finally, the vector $\bb$
is a projected full-rank advection computed via equation \ref{eq:fulladvect}. Schemes for solving the NNLS problem efficiently are well-known, including the standard Lawson-Hanson method \cite{lawson1995solving}, which has $O(rTn^3)$ complexity, and the ``fast'' nonnegative least squares method (FNNLS) introduced by Bro and de Jong \cite{bro1997fast}, which achieves a constant speedup over Lawson-Hanson. Provided that the number of cubature points $n$ remains much smaller than the full-space resolution $N$, these timings are acceptable. We review two different techniques for selecting the cubature points: {\em greedy} selection and {\em importance sampled} selection.
\subsubsection{Greedy Cubature Selection}
We see from equation \ref{eq:cubature} that given a set $\fancyS$ of cubature points, we can calculate the corresponding weights from the weight vector $\ww$. However, the question remains of how to select
the set $\fancyS$ in the first place. The technique of greedy selection was introduced by An et al.~in \cite{An:2008}. Suppose for the sake of argument we have already selected a set of cubature points, giving us a preliminary set $\fancyS$, yet the residual $\rr = \bb - \boldA \ww$ has a significant
magnitude $\left \lVert \rr \right \rVert_2 > \epsilon$ greater than a desired tolerance. To decrease the error, we must add a new cubature point to $\fancyS$, which extends the matrix $\boldA$ by appending a new column.
This will then update the new residual and decrease the error. The ``greedy'' solution is to select a cubature point $p$ whose corresponding column $\aaa_p$ maximally projects onto the residual $\rr$; i.e., we
wish to maximize the dot product $\aaa_p \cdot \rr$. Once such a point $p$ is found, we add it to our set $\fancyS$ and update the residual. We repeat this process until the residual has a magnitude smaller than
the desired tolerance $\epsilon$. The overall complexity of the greedy approach is $O(rTn^4)$.

\begin{algorithm}
\caption{Greedy Cubature Selection}
\begin{algorithmic}[1]
\Function{GreedyCubature}{$\boldA$, $\bb$, $\epsilon$}
\State $\fancyS \gets \emptyset$
\State $\rr \gets \bb$
\While {$\left \lVert \rr \right \rVert_2 > \epsilon$}
\State $\fancyC \gets \Call{SelectCandidatePoints}{\fancyS}$
\State $p \gets \argmax_{p \in \fancyC} {\aaa_p \cdot \rr}$
\State $\fancyS \gets \fancyS \cup \{p\}$
\State $\ww \gets \Call{NNLS}{\boldA_{\fancyS},\bb}$
\State $\rr \gets \bb - \boldA_{\fancyS} \ww$
\EndWhile
\Return{$(\fancyS$, $\ww$)}
\EndFunction
\end{algorithmic}
\label{alg:greedy}
\end{algorithm}

\subsubsection{Importance Samped Cubature Selection}
The technique of importance sampled cubature selection was proposed by Kim and Delaney in \cite{Kim2013}. In the greedy algorithm, because the matrices $A_S$ between one iteration and the next differ only
by one column, much of the work becomes redundant. Hence, instead of adding only one cubature point at a time, in the importance sampled selection strategy, we add $C$ points at each iteration. The strategy
for selecting the $C$ points is done through importance sampling according to the following probability mass function:

\begin{equation}
\label{eq:pmf}
	\text{PMF}(p) = R \frac{\lvert \aaa_p \cdot \rr \rvert}{\rr \cdot \rr}
\end{equation}
Here, $R$ denotes the number of points not yet in the cubature set. 

To implement the importance sampling algorithm, we replace lines 5--7 of the Algorithm \ref{alg:greedy} with the following:

\begin{algorithm}
\caption{Importance Sampled Cubature Selection}
\begin{algorithmic}[1]
\Function{ImportanceSampledCubature}{$C$}
\While {$C$ points have not been added to $\fancyS$}
\State {Randomly select a candidate $p \notin \fancyS$}
\State {Add $p$ to $\fancyS$ with probability $\text{PMF}(p)$}
\EndWhile
\EndFunction
\end{algorithmic}
\label{alg:importance}
\end{algorithm}

The importance sampling algorithm has the advantage of a superior run-time complexity: asymptotically, it runs in $O(rTn^3)$ time. Other approaches such as nonnegative hard thresholding pursuit (NN-HTP) \cite{von2013efficient}
and alternating direction method of multipliers (ADMM) \cite{pan2015subspace} have been explored as well, but they are outside the scope of this dissertation.