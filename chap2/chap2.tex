\chapter[Related Work]{Related Work}
\label{chap:chap2}
\section{Sonification and Generative Art}

The question of mapping computational fluid dynamics data into sound belongs to the intersection of the domains of {\em sonification} and {\em generative art}. Sources vary in agreement on the definition of sonification. According to Hermann, sonification is ``the technique of rendering sound in response to data and interactions.'' \cite{hermann2011sonification} Kramer et al.~define it as ``the use of nonspeech audio to convey information.'' \cite{kramer2010sonification} We shall make a distinction between {\em scientific} sonification, which aims primarily toward conveying information clearly, and {\em musical sonification}, which aims primarily toward aesthetically useful generation of music. There are many possible strategies for sonification, including audification, parameter mapping sonification, and model-based sonification. \cite{hermann2011sonification} We shall expand upon the meaning of these terms in \S\cite{sec:background}.

Generative art is sometimes thought of in very general terms. For instance, Boden specifies eleven different categories of generative art: electronic art, computer art, computer-assisted art digital art, generative art, computer-generated art, evolutionary art, robot art, interactive art, computer-interactive art, and virtual reality art. \cite{boden2009generative}. In the text, however, we prefer a more narrow definition of generative art as art which has been created through the design and use of a computational system.

One of the main attractions of this compositional style is the possibility of novel discoveries that go beyond what the artist may otherwise have been able to conceive: ``The computer can allow a composer to write music that goes beyond that which she is already capable of.'' \cite{roads2015composing}

\subsection{Background}
\label{sec:background}
The use of natural processes as a technique in music composition has been well known for centuries, from the Greek's use of harmonic proportions in tuning to Mozart's dice music. The twentieth century saw an increased interest in formalized mathematical systems as part of musical composition. Composers and theorists such as Schillinger, Stockhausen, Xenakis, Cage, Lucier, Lewin, and many others explored different systematic ways of generating sonic art. In the twenty-first century, with the explosion of the idea of ``big data,'' music and sound has been generated from all sorts of data sets such as astronomical measurements, seismographs, web traffic, and more. Gradually, an important question emerged: is the composer focused primarily with the listener's musical experience, or is it the intent that the music inform the listener of features of the underlying data? Such a question touches the tip of the iceberg of many challenging philosophical questions, such as whether music can even have any external meaning, or the distinction between the artist's intention and the listener's response. While we shall only explore a few of these ideas in some detail, the interested reader can find more thorough discussions in \todo{citation}.

In the earlier forms of sonification, the data itself tended to be the most prominent feature. Indeed, sonification was viewed as an offshoot of data visualization. Thus, for example, the technique of audification, in which data values are mapped directly to sound pressure levels, by maintaining a literal connection between the data and the sound, serves as a direct form of sonification. Audification has been used in especially for natural phenomenon that generate time series, such as seismology, or stock market prices. Parameter mapping, by contrast, is more of a second-order technique in which one parameter of the data is mapped to a parameter of sound, such as pitch, or volume. For example, a series of data representing temperature values might be mapped to pitch in such a way that higher temperature sounds as higher pitch, and vice versa. Although the data is not literally being interpreted as a waveform, the analogy of human perception still makes the connection between the data and sound tangible. Finally, more abstract forms of sonification, such as model-based techniques, in which a time-varying process is devised that connects the data and sound together, can strain the perceptual limits of the link between the data and the sound, although formally speaking, it still is present. Kramer proposes a ``semiotic'' spectrum from analogic to symbolic sonifications, with audification at the extreme analogic end as closest to the data, parameter mapping in the middle, and model-based techniques at the symbolic extreme.

\subsection{Model-Based Sonification}
\label{sec:modelbased}
As our particular research strategy resembles the category of model-based sonification (MBS) the most closely, we review this technique in some detail.
According to Hermann, ``Model-Based Sonification is a sonification technique that takes a
particular look at how acoustic responses are generated in response to the userâ€™s actions, and
offers a framework to govern how these insights can be carried over to data sonification. As
a result, Model-Based Sonification demands the creation of processes that involve the data in
a systematic way, and that are capable of evolving in time to generate an acoustic signal.'' \cite{hermann2011sonification} The latter quality of evolving in time to generate an acoustic signal interests us the most, as the time evolution in such models is often governed systematically according to physical principles, making it an ideal candidate for a sonification of a physical process such as fluid dynamics. After exploring some fundamental ideas in human perception and modes of listening, Hermann concludes that a successful sonification should satisfy five properties, paraphrased thusly:
\begin{itemize}
	\item Ubiquity: Each interaction with data should produce a sound.
	\item Invariance of binding mechanism: The laws producing the sound from the data should not depend on the data itself, much in the same way that the
	laws of physics do not depend on the specific objects that they govern.
	\item Immediate response: Each interaction with the data should produce an immediate response in order to mirror as closely as possible interactions with objects in the real world
	\item Sonic variability: Sonifications should vary according to subtle changes in state and input.
	\item Information richness: Sonifications should be nontrivial.
\end{itemize}
Essentially, these properties guarantee that a sonification mimics interactions in the real world with data and sound. This ties in with a more analytical mode of listening---for instance, shaking a wrapped birthday present to try to determine its contents from the corresponding sound, rather than listening to the sound of the jostling present as a musical event. By using the technique of MBS, Hermann concludes that these properties will in fact automatically be satisfied. The basic principle, thus, is to design something analogous to the laws of physics, but as a sonification system of data. The model then becomes the rules by which the data is mapped to the sound and unfolds over time, much as the laws of physics determine the rules by which objects interact with one another in space and time.

Hermann also identifies six key components that aid in the design of a MBS system: setup, model dynamics, excitation, initial state, link-variables, and listener characteristics. It is also useful to keep as separate abstract concepts the data space, in which the data lives; the model space, in which the sonification ```laws'' live; the sound space, in which the actual sound itself lives; and the listener space, in which the listener's reaction to the sound lives. The model setup, thus, takes data from the data space, and maps it to the space of a dynamical model with time-varying components that create the corresponding sound in the sound space. 

As a general example, suppose that the data can be interpreted as a collection of vectors $\{\xx_j\}, j=1,\dots,N$ living in the $d$-dimensional vector space $\R^d$. (For instance, more concretely, the data might comprise the temperature, barometric pressure, and humidity in Santa Barbara for each day in the year 2017, meaning $d=3$ and the collection has $N=365$ elements.) Such a re-interpretation of the data abstractly in a mathematical space is the primary goal of the model setup. The advantage of such an approach is the ubiquity of mathematical tools in linear algebra that will play nicely only if we view our data through the lens of a vector space.

Given our model setup, we next turn to the model dynamics. Since our goal is to drive a sound-based model over time, it is natural to consider a time-varying evolution to our model. In other words, we consider the model not as a static configuration but as a dynamical system. Mathematically speaking, if we write ${\bolds}(t)$ to denote the state of the model at time $t$, then we would like a differential equation to govern the time evolution---e.g.,

\begin{equation}
\frac{d{\bolds}}{dt} = f({\bolds}(t), t)
\end{equation}

The function $f$ here determines the model dynamics. Already the connection with physics is tempting. Although the function $f$ in principle is arbitrary, the ideas of conservation of energy, gravity and buoyancy, friction, dissipation, and other physical principles provide an attractive palette from which to borrow. 

The excitation can be thought of as analogous perturbations to the balance of forces in a dynamical system. In a general dynamical system, an excitation might be obtained through interaction, as a user adds external forces to the physics through a keystroke or mouse click. For example, in a fluid simulation, a user might add a force to the fluid velocity field by clicking and dragging the mouse through a region. Other possible fluid examples include altering the fluid's viscosity or introducing a buoyant external force. 

The model setup is analogous to the initial and boundary conditions of a dynamical system. For example, a typical first-order ordinary differential equation $\frac{dx}{dt} = f(x(t), t)$ defines an entire family of possible solutions $x$. Only by specifying an initial condition $x(0) = x_0$ do we uniquely determine which trajectory to take. More general differential equations, in addition to requiring initial conditions, may also require boundary conditions to determine edge behaviors. For instance, in a fluid simulation, we frequently use the ``no-slip'' boundary condition, meaning that at a solid boundary, the fluid has zero velocity relative to the boundary. These parameters can greatly influence the resulting simulation, but the designer typically through experience and experimentation has knowledge of which conditions to use in order to obtain the desired physical effect.

Link variables form the bridge between the dynamical process of the model and actual sound. In the physical world, these connections often happen literally. For instance, if we model the rigid vibrations of a metal plate using a spring-mass system, the corresponding air pressure variations it induces lead directly to an audible sound signal. Naturally, this isn't so much of a sonification as an actual physical calculation of sound, but the analogy remains useful for more abstract sonifications. In practice, link variables are often more of a second-order connection, much as in parameter-mapping sonification. For example, we might map the overall energy of a system to a musical loudness. A common challenge with MBS rears its head here, as depending on the complexity of the model, a real-time sound output may not be feasible. In particular, if the link variables are conceived of at an audio sample rate, in order to maintain high-quality sound rendering, we require at least 44100 samples to be computed per second, which may be computationally infeasible. Indeed, the audiovisual system proposed in this dissertation cannot be run in real-time using the available computing power as of writing in 2017. The tradeoffs of real-time vs. nonreal-time composition will be discussed in section $\todo{cite section}$.

Listener characteristics aim to understand better the desired mode of listening for which the composer aims. For example, shall the listener perceive herself to be ``inside'' the system, or an outside observer? Should the sound be perceived as a single source, or an entire soundscape? Many of these questions have design answers in the form of the spatializatin of the audio output. For instance, by using careful panning techniques, the composer can create many different desired perceptions in the listener.

\subsection{Aesthetics}
A continual dilemma in generative art is the conflict between the level of rigor of the underlying formal system and the human perception of its output. Some artists (e.g. Milton Babbitt) take the dogmatic position that the logic of the system trumps the general perception of the output. \cite{babbitt1958cares}  However, others such as K{\v{r}}enek have taken a more careful middle ground, arguing that the existence of an aesthetically coherent system of rules is no guarantee that an aesthetically coherent artistic result will perceptibly emerge: ``We cannot take the bare logical coherence of a musical `axiomatic' system as the sole criterion of its soundness! \dots The outstanding characteristic of music [is] its independence from the linguistic limitations of general logic.'' \cite{kvrenek1939music}